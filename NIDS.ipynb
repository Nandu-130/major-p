{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nandu-130/major-p/blob/main/NIDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIaCBFfQegIS",
        "outputId": "b1f33006-2288-4c16-8699-ddcf92df2f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adhNl7yNgmKn",
        "outputId": "cefe12b7-0fee-455a-8488-28418cea8cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Colab cell 1 - install / imports\n",
        "!pip install --quiet torch torchvision torchaudio scikit-learn pandas numpy matplotlib\n",
        "\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSONOG3bgt_5"
      },
      "outputs": [],
      "source": [
        "# Colab cell 2 - Utilities: MAD outlier removal, preprocessing functions\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
        "\n",
        "def mad_filter(df, numeric_cols, multiplier=10.0):\n",
        "    \"\"\"\n",
        "    Apply Median Absolute Deviation (MAD) filtering to remove outliers.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    for col in numeric_cols:\n",
        "        med = df[col].median()\n",
        "        mad = (df[col] - med).abs().median()\n",
        "        sigma_hat = 1.4826 * mad\n",
        "        if sigma_hat == 0:\n",
        "            continue\n",
        "        threshold = multiplier * sigma_hat\n",
        "        df = df[(df[col] - med).abs() <= threshold]\n",
        "    return df\n",
        "\n",
        "def preprocess_df(df, categorical_cols, numeric_cols, label_col):\n",
        "    \"\"\"\n",
        "    Perform preprocessing steps:\n",
        "      1. MAD outlier filtering\n",
        "      2. One-hot encode categorical features\n",
        "      3. Min–Max scale numeric features\n",
        "      4. Label-encode target column\n",
        "    \"\"\"\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    # --- Outlier removal ---\n",
        "    df = mad_filter(df, numeric_cols, multiplier=10.0)\n",
        "\n",
        "    # --- One-hot encode categorical features ---\n",
        "    if categorical_cols:\n",
        "        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "        cat_arr = ohe.fit_transform(df[categorical_cols])\n",
        "        cat_cols = ohe.get_feature_names_out(categorical_cols)\n",
        "        df_cat = pd.DataFrame(cat_arr, columns=cat_cols, index=df.index)\n",
        "        df = pd.concat([df.drop(columns=categorical_cols), df_cat], axis=1)\n",
        "\n",
        "    # --- Min–Max scaling for numeric features ---\n",
        "    scaler = MinMaxScaler()\n",
        "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # --- Label encode target column ---\n",
        "    le = LabelEncoder()\n",
        "    df[label_col] = le.fit_transform(df[label_col])\n",
        "\n",
        "    return df, scaler, le\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUh1keBCgwYx",
        "outputId": "2d88a56c-7738-4fad-ba86-ea4e50a1fc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ NSL-KDD dataset loaded successfully!\n",
            "Shape: (148517, 42)\n",
            "Label counts:\n",
            " label\n",
            "Normal    77054\n",
            "DoS       53385\n",
            "Probe     14077\n",
            "R2L        3880\n",
            "U2R         119\n",
            "Other         2\n"
          ]
        }
      ],
      "source": [
        "# ✅ Colab Cell 3 - Load NSL-KDD dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Download dataset if not present\n",
        "if not os.path.exists(\"/content/KDDTrain+.txt\"):\n",
        "    !wget -q https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt\n",
        "    !wget -q https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest+.txt\n",
        "\n",
        "# Define column names\n",
        "cols = [\n",
        "    'duration','protocol_type','service','flag','src_bytes','dst_bytes','land','wrong_fragment',\n",
        "    'urgent','hot','num_failed_logins','logged_in','num_compromised','root_shell',\n",
        "    'su_attempted','num_root','num_file_creations','num_shells','num_access_files',\n",
        "    'num_outbound_cmds','is_host_login','is_guest_login','count','srv_count',\n",
        "    'serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate','same_srv_rate',\n",
        "    'diff_srv_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count',\n",
        "    'dst_host_same_srv_rate','dst_host_diff_srv_rate','dst_host_same_src_port_rate',\n",
        "    'dst_host_srv_diff_host_rate','dst_host_serror_rate','dst_host_srv_serror_rate',\n",
        "    'dst_host_rerror_rate','dst_host_srv_rerror_rate','label','difficulty'\n",
        "]\n",
        "\n",
        "# Load train and test data\n",
        "train_df = pd.read_csv(\"/content/KDDTrain+.txt\", names=cols)\n",
        "test_df  = pd.read_csv(\"/content/KDDTest+.txt\", names=cols)\n",
        "\n",
        "# Merge both\n",
        "df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "# Drop unused column\n",
        "df.drop(columns=['difficulty'], inplace=True, errors='ignore')\n",
        "\n",
        "# ---- Label grouping (multiclass example) ----\n",
        "dos = ['back','land','neptune','pod','smurf','teardrop','mailbomb','apache2','processtable','udpstorm']\n",
        "probe = ['satan','ipsweep','nmap','portsweep','mscan','saint']\n",
        "r2l = ['guess_passwd','ftp_write','imap','phf','multihop','warezmaster','warezclient','spy','xlock','xsnoop',\n",
        "        'snmpguess','snmpgetattack','httptunnel','sendmail','named']\n",
        "u2r = ['buffer_overflow','loadmodule','rootkit','perl','sqlattack','xterm','ps']\n",
        "\n",
        "def map_label(x):\n",
        "    if x in dos: return 'DoS'\n",
        "    elif x in probe: return 'Probe'\n",
        "    elif x in r2l: return 'R2L'\n",
        "    elif x in u2r: return 'U2R'\n",
        "    elif x == 'normal': return 'Normal'\n",
        "    else: return 'Other'\n",
        "\n",
        "df['label'] = df['label'].apply(map_label)\n",
        "\n",
        "print(\"✅ NSL-KDD dataset loaded successfully!\")\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Label counts:\\n\", df['label'].value_counts().to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOM-RNGpn3zi",
        "outputId": "d0216451-5da8-4f01-db46-3f7e94ad1ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed shape: (104428, 119)\n",
            "Classes (encoded): {np.int64(0): np.int64(38047), np.int64(1): np.int64(56093), np.int64(2): np.int64(7174), np.int64(3): np.int64(3051), np.int64(4): np.int64(63)}\n",
            "Train/test: (73099, 118) (31329, 118)\n"
          ]
        }
      ],
      "source": [
        "# Colab cell 4 - Prepare train/test and split by class (we will train a GAN per class to generate minority samples)\n",
        "LABEL_COL = 'label'\n",
        "# Identify categorical/numeric columns based on original dataframe dtypes\n",
        "# We need to make sure these are correctly identified before preprocessing\n",
        "categorical_cols = [c for c in df.columns if df[c].dtype == object and c != LABEL_COL]\n",
        "numeric_cols = [c for c in df.columns if c not in categorical_cols + [LABEL_COL]]\n",
        "\n",
        "# Explicitly convert categorical columns to string type before preprocessing\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].astype(str)\n",
        "\n",
        "# Now call preprocess_df with the dataframe where categorical columns are strings\n",
        "df_proc, scaler, labelenc = preprocess_df(df, categorical_cols, numeric_cols, LABEL_COL)\n",
        "print(\"Processed shape:\", df_proc.shape)\n",
        "X = df_proc.drop(columns=[LABEL_COL]).values.astype(np.float32)\n",
        "y = df_proc[LABEL_COL].values\n",
        "classes, counts = np.unique(y, return_counts=True)\n",
        "print(\"Classes (encoded):\", dict(zip(classes, counts)))\n",
        "\n",
        "# Split train/test (stratify to preserve class ratios)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "print(\"Train/test:\", X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAIMAI66oKou"
      },
      "outputs": [],
      "source": [
        "# Colab cell 5 - PyTorch Dataset wrapper\n",
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = None if y is None else torch.from_numpy(y).long()\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None: return self.X[idx]\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(NumpyDataset(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd-yrfg4oOxg"
      },
      "outputs": [],
      "source": [
        "# Colab cell 6 - WGAN-GP model definitions (MLP generator/discriminator)\n",
        "# We'll use an MLP generator and discriminator suitable for tabular flows.\n",
        "# Latent dim and hidden sizes chosen following the paper (latent 50, hidden around 80). :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "latent_dim = 50\n",
        "hidden_dim = 80\n",
        "data_dim = X_train.shape[1]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=latent_dim, out_dim=data_dim, hidden=hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, hidden),\n",
        "            nn.BatchNorm1d(hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.BatchNorm1d(hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_dim),\n",
        "            nn.Sigmoid()  # because features are min-max scaled [0,1]\n",
        "        )\n",
        "    def forward(self, z): return self.net(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_dim=data_dim, hidden=hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x).view(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrlblynnoVzF",
        "outputId": "026c58f5-6a28-4b7c-e3cb-254a47406d2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train class counts: {np.int64(0): np.int64(26632), np.int64(1): np.int64(39265), np.int64(2): np.int64(5022), np.int64(3): np.int64(2136), np.int64(4): np.int64(44)}\n",
            "Training WGAN-GP for class 0 (n=26632) ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1/150] D_loss -4.9156 G_loss 5.1020 gp 0.0490\n",
            "[Epoch 50/150] D_loss -0.3038 G_loss -0.0956 gp 0.0012\n",
            "[Epoch 100/150] D_loss -0.2432 G_loss -0.0383 gp 0.0011\n",
            "[Epoch 150/150] D_loss -0.2124 G_loss -0.0396 gp 0.0008\n",
            "Training WGAN-GP for class 1 (n=39265) ...\n",
            "[Epoch 1/150] D_loss -4.8116 G_loss 4.8528 gp 0.0469\n",
            "[Epoch 50/150] D_loss -0.1165 G_loss -0.1455 gp 0.0009\n",
            "[Epoch 100/150] D_loss -0.0808 G_loss -0.6735 gp 0.0004\n",
            "[Epoch 150/150] D_loss -0.0740 G_loss -0.7483 gp 0.0006\n",
            "Training WGAN-GP for class 2 (n=5022) ...\n",
            "[Epoch 1/150] D_loss 4.4121 G_loss 0.1952 gp 0.4785\n",
            "[Epoch 50/150] D_loss -0.8179 G_loss -0.0020 gp 0.0031\n",
            "[Epoch 100/150] D_loss -0.4098 G_loss -0.3744 gp 0.0016\n",
            "[Epoch 150/150] D_loss -0.3611 G_loss -0.2990 gp 0.0026\n",
            "Training WGAN-GP for class 3 (n=2136) ...\n",
            "[Epoch 1/150] D_loss 6.4271 G_loss 0.1525 gp 0.6562\n",
            "[Epoch 50/150] D_loss -2.8346 G_loss 2.4067 gp 0.0192\n",
            "[Epoch 100/150] D_loss -1.1187 G_loss 0.5414 gp 0.0041\n",
            "[Epoch 150/150] D_loss -0.5957 G_loss 0.1203 gp 0.0037\n",
            "Training WGAN-GP for class 4 (n=44) ...\n",
            "[Epoch 1/150] D_loss 8.4820 G_loss -0.0830 gp 0.8443\n",
            "[Epoch 50/150] D_loss -0.9616 G_loss 1.3249 gp 0.0536\n",
            "[Epoch 100/150] D_loss -4.7037 G_loss 4.8031 gp 0.0406\n",
            "[Epoch 150/150] D_loss -4.9065 G_loss 5.0793 gp 0.0478\n"
          ]
        }
      ],
      "source": [
        "# Colab cell 7 - WGAN-GP training loop (per-class)\n",
        "# Implements gradient penalty term. We'll train a generator per class (as the paper did for BEGAN), but using WGAN-GP.\n",
        "# The paper suggests generating separate generators per class and generating synthetic data per class. :contentReference[oaicite:4]{index=4}\n",
        "\n",
        "def gradient_penalty(D, real, fake, device):\n",
        "    alpha = torch.rand(real.size(0), 1, device=device)\n",
        "    alpha = alpha.expand_as(real)\n",
        "    interpolated = alpha * real + (1 - alpha) * fake\n",
        "    interpolated.requires_grad_(True)\n",
        "    d_interpolated = D(interpolated)\n",
        "    grads = torch.autograd.grad(outputs=d_interpolated, inputs=interpolated,\n",
        "                                grad_outputs=torch.ones_like(d_interpolated),\n",
        "                                create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "    grads = grads.view(grads.size(0), -1)\n",
        "    gp = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gp\n",
        "\n",
        "def train_wgangp_for_class(X_class, n_epochs=200, batch_size=256, lr=1e-4, n_critic=5, lambda_gp=10.0):\n",
        "    G = Generator().to(device)\n",
        "    D = Discriminator().to(device)\n",
        "    opt_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "    opt_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "\n",
        "    dataset = DataLoader(NumpyDataset(X_class), batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for i, real_batch in enumerate(dataset):\n",
        "            real = real_batch.to(device)\n",
        "            # train D more steps\n",
        "            for _ in range(n_critic):\n",
        "                z = torch.randn(batch_size, latent_dim, device=device)\n",
        "                fake = G(z).detach()\n",
        "                D_real = D(real).mean()\n",
        "                D_fake = D(fake).mean()\n",
        "                gp = gradient_penalty(D, real, fake, device)\n",
        "                d_loss = D_fake - D_real + lambda_gp * gp\n",
        "                opt_D.zero_grad(); d_loss.backward(); opt_D.step()\n",
        "\n",
        "            # train G\n",
        "            z = torch.randn(batch_size, latent_dim, device=device)\n",
        "            fake = G(z)\n",
        "            g_loss = -D(fake).mean()\n",
        "            opt_G.zero_grad(); g_loss.backward(); opt_G.step()\n",
        "\n",
        "        # optional: print progress every some epochs\n",
        "        if (epoch+1) % 50 == 0 or epoch == 0:\n",
        "            print(f\"[Epoch {epoch+1}/{n_epochs}] D_loss {d_loss.item():.4f} G_loss {g_loss.item():.4f} gp {gp.item():.4f}\")\n",
        "\n",
        "    return G, D\n",
        "\n",
        "# We'll build a generator per class and store them\n",
        "generators = {}\n",
        "unique_train_classes, counts = np.unique(y_train, return_counts=True)\n",
        "print(\"Train class counts:\", dict(zip(unique_train_classes, counts)))\n",
        "for cls in unique_train_classes:\n",
        "    # pick data rows belonging to this class\n",
        "    Xc = X_train[y_train == cls]\n",
        "    print(f\"Training WGAN-GP for class {cls} (n={len(Xc)}) ...\")\n",
        "    # if class too small, lower batch or increase epochs. For demo keep epochs small.\n",
        "    Gc, Dc = train_wgangp_for_class(Xc.astype(np.float32), n_epochs=150, batch_size=min(256, max(32, len(Xc)//4)))\n",
        "    generators[cls] = Gc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6OqZrclosF4",
        "outputId": "869bd84a-ae18-4f03-8b8c-42f1992813f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 12633 synthetic samples for class 0\n",
            "Generating 34243 synthetic samples for class 2\n",
            "Generating 37129 synthetic samples for class 3\n",
            "Generating 39221 synthetic samples for class 4\n",
            "Augmented train shape: (196325, 118)\n"
          ]
        }
      ],
      "source": [
        "# Colab cell 8 - Generate synthetic samples per class and augment training set\n",
        "# Decide how many synthetic samples per class to create. For minority classes, we generate more.\n",
        "def generate_synthetic_for_class(G, n_samples):\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        Z = torch.randn(n_samples, latent_dim, device=device)\n",
        "        synth = G(Z).cpu().numpy()\n",
        "    return synth\n",
        "\n",
        "# Example policy: bring each class up to the size of the largest class (simple balance strategy)\n",
        "train_counts = {c: int((y_train==c).sum()) for c in unique_train_classes}\n",
        "max_count = max(train_counts.values())\n",
        "aug_X_list = []\n",
        "aug_y_list = []\n",
        "for cls in unique_train_classes:\n",
        "    need = max_count - train_counts[cls]\n",
        "    if need <= 0:\n",
        "        continue\n",
        "    print(f\"Generating {need} synthetic samples for class {cls}\")\n",
        "    Gc = generators[cls]\n",
        "    synth = generate_synthetic_for_class(Gc, need)\n",
        "    aug_X_list.append(synth)\n",
        "    aug_y_list.append(np.full(len(synth), cls, dtype=int))\n",
        "\n",
        "if aug_X_list:\n",
        "    X_synth = np.vstack(aug_X_list)\n",
        "    y_synth = np.concatenate(aug_y_list)\n",
        "    X_train_aug = np.vstack([X_train, X_synth])\n",
        "    y_train_aug = np.concatenate([y_train, y_synth])\n",
        "else:\n",
        "    X_train_aug = X_train.copy()\n",
        "    y_train_aug = y_train.copy()\n",
        "\n",
        "print(\"Augmented train shape:\", X_train_aug.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n69e4feoxHg",
        "outputId": "5fddb314-b774-4229-f2fb-1f937253c292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AE Epoch 1 Loss 0.014734\n",
            "AE Epoch 50 Loss 0.000423\n",
            "AE Epoch 100 Loss 0.000402\n",
            "AE Epoch 150 Loss 0.000389\n",
            "AE Epoch 200 Loss 0.000385\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=118, out_features=80, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=80, out_features=50, bias=True)\n",
              "  (3): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Colab cell 9 - Autoencoder training (symmetric autoencoder similar to discriminator/BEGAN's AE)\n",
        "# The paper used AE as feature extractor (same architecture as discriminator) with hidden 80 and latent 50. :contentReference[oaicite:5]{index=5}\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=data_dim, hidden=hidden_dim, latent=latent_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, latent),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        xrec = self.decoder(z)\n",
        "        return xrec\n",
        "\n",
        "ae = Autoencoder().to(device)\n",
        "ae_opt = optim.Adam(ae.parameters(), lr=1e-3)\n",
        "ae_loss_fn = nn.MSELoss()\n",
        "\n",
        "ae_loader = DataLoader(NumpyDataset(X_train_aug), batch_size=256, shuffle=True)\n",
        "# train AE\n",
        "for epoch in range(200):  # paper used up to 300 epochs; early stopping condition could be added\n",
        "    epoch_loss = 0.0\n",
        "    for xb in ae_loader:\n",
        "        xb = xb.to(device)\n",
        "        xr = ae(xb)\n",
        "        loss = ae_loss_fn(xr, xb)\n",
        "        ae_opt.zero_grad(); loss.backward(); ae_opt.step()\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "    epoch_loss /= len(ae_loader.dataset)\n",
        "    if (epoch+1) % 50 == 0 or epoch == 0:\n",
        "        print(f\"AE Epoch {epoch+1} Loss {epoch_loss:.6f}\")\n",
        "# Extract encoder (to be frozen for classifier)\n",
        "encoder = ae.encoder\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "encoder.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujhwg9zOo4Yn"
      },
      "outputs": [],
      "source": [
        "# Colab cell 10 - Build classifiers (DNN, 1D-CNN, LSTM) that use encoder as front-end (DNNAE / CNNAE)\n",
        "# DNNAE: encoder output -> dense classifier\n",
        "class DNNAE(nn.Module):\n",
        "    def __init__(self, encoder, latent_dim=latent_dim, n_classes=len(unique_train_classes)):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, n_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            z = self.encoder(x)\n",
        "        return self.classifier(z)\n",
        "\n",
        "# CNNAE: since data is tabular, emulate 1D-CNN by reshaping to (batch, channels=1, seq_len=data_dim)\n",
        "class CNNAE(nn.Module):\n",
        "    def __init__(self, encoder, seq_len=data_dim, n_classes=len(unique_train_classes)):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, n_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        # pass through encoder first (frozen)\n",
        "        with torch.no_grad():\n",
        "            z = self.encoder(x)\n",
        "        # reshape z to (batch, 1, latent_dim)\n",
        "        z = z.unsqueeze(1)\n",
        "        c = self.conv(z).view(z.size(0), -1)\n",
        "        return self.fc(c)\n",
        "\n",
        "# For LSTM, paper omitted encoder; we build an LSTM classifier on raw features shaped as sequences\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=data_dim, hidden_size=64, num_layers=2, n_classes=len(unique_train_classes)):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, n_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, feature_dim) -> treat features as sequence length\n",
        "        x_seq = x.unsqueeze(-1)  # (batch, seq_len, 1)\n",
        "        out, _ = self.lstm(x_seq)\n",
        "        last = out[:, -1, :]\n",
        "        return self.fc(last)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNpxJHhVo78f"
      },
      "outputs": [],
      "source": [
        "# Colab cell 11 - Train classifier function + evaluate\n",
        "def train_classifier(model, X_tr, y_tr, X_val, y_val, epochs=100, lr=1e-3):\n",
        "    model = model.to(device)\n",
        "    opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    ds = DataLoader(NumpyDataset(X_tr, y_tr), batch_size=256, shuffle=True)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for xb, yb in ds:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "        total_loss /= len(ds.dataset)\n",
        "        if (epoch+1) % 50 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} loss {total_loss:.4f}\")\n",
        "    # eval\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred_logits = model(torch.from_numpy(X_val).float().to(device))\n",
        "        preds = pred_logits.argmax(dim=1).cpu().numpy()\n",
        "    print(classification_report(y_val, preds, zero_division=0))\n",
        "    return model\n",
        "\n",
        "# Prepare validation/test sets (we'll use X_test / y_test processed earlier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq_bCLVepAdS",
        "outputId": "d9a61a7b-699d-48dd-f5f4-20b7a13e9118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training DNNAE...\n",
            "Epoch 1/100 loss 0.1924\n",
            "Epoch 50/100 loss 0.0091\n",
            "Epoch 100/100 loss 0.0065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     11415\n",
            "           1       0.99      1.00      0.99     16828\n",
            "           2       0.99      0.98      0.98      2152\n",
            "           3       0.95      0.90      0.92       915\n",
            "           4       0.57      0.21      0.31        19\n",
            "\n",
            "    accuracy                           0.99     31329\n",
            "   macro avg       0.90      0.82      0.84     31329\n",
            "weighted avg       0.99      0.99      0.99     31329\n",
            "\n",
            "\n",
            "Training CNNAE...\n",
            "Epoch 1/100 loss 0.2197\n",
            "Epoch 50/100 loss 0.0071\n",
            "Epoch 100/100 loss 0.0058\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     11415\n",
            "           1       0.99      1.00      0.99     16828\n",
            "           2       0.98      0.98      0.98      2152\n",
            "           3       0.98      0.87      0.92       915\n",
            "           4       0.60      0.16      0.25        19\n",
            "\n",
            "    accuracy                           0.99     31329\n",
            "   macro avg       0.91      0.80      0.83     31329\n",
            "weighted avg       0.99      0.99      0.99     31329\n",
            "\n",
            "\n",
            "Training LSTM (naive)...\n",
            "Epoch 1/100 loss 1.1657\n",
            "Epoch 50/100 loss 0.0108\n",
            "Epoch 100/100 loss 0.0061\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 18.66 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.47 GiB is free. Process 5625 has 1.26 GiB memory in use. Of the allocated memory 1010.89 MiB is allocated by PyTorch, and 145.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2848077989.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mmodel_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m model_lstm = train_classifier(\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mmodel_lstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mX_train_aug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_aug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2689176963.py\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(model, X_tr, y_tr, X_val, y_val, epochs, lr)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mpred_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2888421173.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# x shape: (batch, feature_dim) -> treat features as sequence length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mx_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, seq_len, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             result = _VF.lstm(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.66 GiB. GPU 0 has a total capacity of 14.74 GiB of which 13.47 GiB is free. Process 5625 has 1.26 GiB memory in use. Of the allocated memory 1010.89 MiB is allocated by PyTorch, and 145.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Colab cell 12 - Train DNNAE, CNNAE, LSTM (epochs = 100)\n",
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Optional: helps reduce CUDA fragmentation\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "print(\"Training DNNAE...\")\n",
        "model_dnnae = DNNAE(encoder)\n",
        "model_dnnae = train_classifier(\n",
        "    model_dnnae,\n",
        "    X_train_aug.astype(np.float32), y_train_aug,\n",
        "    X_test.astype(np.float32), y_test,\n",
        "    epochs=100, lr=1e-3\n",
        ")\n",
        "\n",
        "print(\"\\nTraining CNNAE...\")\n",
        "model_cnnae = CNNAE(encoder)\n",
        "model_cnnae = train_classifier(\n",
        "    model_cnnae,\n",
        "    X_train_aug.astype(np.float32), y_train_aug,\n",
        "    X_test.astype(np.float32), y_test,\n",
        "    epochs=100, lr=1e-3\n",
        ")\n",
        "\n",
        "print(\"\\nTraining LSTM (naive)...\")\n",
        "# If you still hit OOM, uncomment next line to move LSTM to CPU:\n",
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "model_lstm = LSTMClassifier()\n",
        "model_lstm = train_classifier(\n",
        "    model_lstm,\n",
        "    X_train_aug.astype(np.float32), y_train_aug,\n",
        "    X_test.astype(np.float32), y_test,\n",
        "    epochs=100, lr=1e-3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4HVyWhZpJbx",
        "outputId": "b977613a-1ed6-47e5-f43e-a90f788d9497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DNNAE on test:\n",
            "Accuracy: 0.9918924957706917\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     11415\n",
            "           1       0.99      0.99      0.99     16828\n",
            "           2       0.97      0.99      0.98      2152\n",
            "           3       0.91      0.94      0.93       915\n",
            "           4       0.62      0.26      0.37        19\n",
            "\n",
            "    accuracy                           0.99     31329\n",
            "   macro avg       0.90      0.84      0.85     31329\n",
            "weighted avg       0.99      0.99      0.99     31329\n",
            "\n",
            "CNNAE on test:\n",
            "Accuracy: 0.9921478502346069\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     11415\n",
            "           1       0.99      1.00      0.99     16828\n",
            "           2       0.98      0.98      0.98      2152\n",
            "           3       0.97      0.89      0.93       915\n",
            "           4       0.35      0.37      0.36        19\n",
            "\n",
            "    accuracy                           0.99     31329\n",
            "   macro avg       0.86      0.85      0.85     31329\n",
            "weighted avg       0.99      0.99      0.99     31329\n",
            "\n",
            "LSTM on test:\n",
            "Accuracy: 0.9941587666379393\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     11415\n",
            "           1       0.99      1.00      1.00     16828\n",
            "           2       0.99      0.98      0.99      2152\n",
            "           3       0.96      0.91      0.94       915\n",
            "           4       1.00      0.16      0.27        19\n",
            "\n",
            "    accuracy                           0.99     31329\n",
            "   macro avg       0.99      0.81      0.84     31329\n",
            "weighted avg       0.99      0.99      0.99     31329\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Colab cell 13 - Final evaluation summarised (accuracy)\n",
        "def evaluate_model(model, X, y):\n",
        "    model.eval()\n",
        "    # Use a DataLoader for evaluation to avoid OOM errors on large datasets\n",
        "    eval_dataset = NumpyDataset(X, y)\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=256) # Use the same batch size as training\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in eval_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(yb.cpu().numpy())\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(classification_report(all_labels, all_preds, zero_division=0))\n",
        "\n",
        "print(\"DNNAE on test:\")\n",
        "evaluate_model(model_dnnae, X_test.astype(np.float32), y_test)\n",
        "print(\"CNNAE on test:\")\n",
        "evaluate_model(model_cnnae, X_test.astype(np.float32), y_test)\n",
        "print(\"LSTM on test:\")\n",
        "evaluate_model(model_lstm, X_test.astype(np.float32), y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPpJk36Y8z7mwbcPhXe1ztx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}